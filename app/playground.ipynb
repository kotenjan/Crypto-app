{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.genetic import Genetic\n",
    "from ai.interval import Interval\n",
    "from ai.trainer import Trainer\n",
    "from ai.extrema import Extrema\n",
    "from scraper.cryptoreq import CryptoReq\n",
    "from scraper.investreq import InvestReq\n",
    "from scraper.trendreq import TrendReq\n",
    "from scraper.tweetreq import TweetReq\n",
    "from utils.database import Database\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from ai.minmax import Minmax\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you begin\n",
    "\n",
    "Make sure to fill in the cryptocurrency names in the gui on http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database().test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting names\n",
    "\n",
    "When accessing the table with cryptocurrency names, you have the ability to get just names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names(symbols=True)\n",
    "for name, symbol in names:\n",
    "    print(name, symbol)\n",
    "\n",
    "print()\n",
    "\n",
    "names = Database().get_names()\n",
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency price values\n",
    "\n",
    "The initial approach to get cryptocurrency price values was to send requests at the coindesk.com server.\n",
    "This approach was later discarded as there was no way of accessing data older than 24 hours.\n",
    "Downloading cryptocurrency data this way no longer works due to the provider changing the format and access method of the data.\n",
    "\n",
    "- `timeout` determines the time it takes to complete one cycle of calls\n",
    "- `short_timeout` determines how long to wait after each call\n",
    "- `scraping_window` sets the interval between `dt.now()` and the starting timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CryptoReq(timeout=30, short_timeout=30, scraping_window=7200).loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data\n",
    "\n",
    "The data is being downloaded from the investing.com servers. User needs to add a custom symbol in the GUI under which is the cryptocurrency accessible. This symbol can be easily found via trancking the site's network communication. With this approach you can download data further in history. This allows the app to start with predictions within a few minutes after the initial start.\n",
    "\n",
    "- `timeout` will determine the time it takes to complete one cycle of calls\n",
    "- `short_timeout` determines how long to wait after each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvestReq(timeout=60, short_timeout=5, scrape_days=30).loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data from Google Search\n",
    "\n",
    "We are able to download popularity of given keyword. Popularity is defined as number of searches defined as a percentage of all of the searches in selected timeframe.\n",
    "\n",
    "- `timeout` determines the time it takes to complete one cycle\n",
    "- `short_timeout` determines how long to wait after each query name is downloaded\n",
    "- `trend_timeout` sets the wait time after each call\n",
    "- `scrape_days` sets the default number of days that are downloaded from timestamp `dt.now() - td(days=scrape_days)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrendReq(timeout=60, short_timeout=25, trend_timeout=5, scrape_days=30).loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading tweets\n",
    "\n",
    "Initially there were attempts to include tweets in the prediction of the value of cryptocurrencies. This approach was later abandoned in favor of the data from Google Search as scraping tweets requires more CPU and has lesser information value. Tweets are not being saved. They are only printed out during scraping. The scraping window is set in tweetreq.py to two hours between now and 2 hours ago.\n",
    "\n",
    "- `timeout` will determine the time it takes to complete one cycle\n",
    "- `short_timeout` determines how long to wait after each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetReq(timeout=60, short_timeout=30).loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting data from backup csv\n",
    "\n",
    "We are able to load backup data after specifying cryptocurrency names in the gui. The saved cryptocurrencies are BTC, ETH, LTC, NEO and XRP.\n",
    "The approach is slower due to the architecture of the methods, which are supposed to prevent any data duplicities and inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "def insert(name, type_name_from, type_name_to):\n",
    "    data = pd.read_csv(f'/workspace/data/{type_name_from}_{name.lower()}.csv')\n",
    "    data[name] = data['count']\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    data = data.set_index('date')\n",
    "    data = pd.DataFrame(data[name])\n",
    "    Database().insert_into_history_table(type_name_to, data)\n",
    "\n",
    "for name in names:\n",
    "    try:\n",
    "        insert(name, 'crypto', 'crypto')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "insert('gold', 'gold', 'gold')\n",
    "insert('vix', 'vix', 'vix')\n",
    "insert('sap', 'sap', 'sap')\n",
    "\n",
    "for name in names:\n",
    "    try:\n",
    "        insert(name, 'trends', 'trends')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data as csv\n",
    "\n",
    "We are able to create backup csv in the same way we are able to load it back into database. Will fix missing values and save resulting dataset in data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "Database().select_all_from_and_fix_missing('vix',    'vix',    'ffill', replace_null=True, limit_area='inside', save_to_csv=True),\n",
    "Database().select_all_from_and_fix_missing('sap',    'sap',    'ffill', replace_null=True, limit_area='inside', save_to_csv=True),\n",
    "Database().select_all_from_and_fix_missing('gold',   'gold',   'ffill', replace_null=True, limit_area='inside', save_to_csv=True),\n",
    "\n",
    "for name in names:\n",
    "    Database().select_all_from_and_fix_missing(name, 'trends', 'linear', replace_null=False, limit_area='inside', save_to_csv=True)\n",
    "    Database().select_all_from_and_fix_missing(name, 'crypto', 'linear', replace_null=True, limit_area='inside', save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "When training model we can specify how many epochs will be trained at most. The training class will automatically end training if the error of the validation set does not improve over 128 epochs.\n",
    "\n",
    "- `names` - names of cryptocurrencies\n",
    "- `window` - number of timestamps per one prediction window\n",
    "- `cycles_t` - number of cycles in trainint model for time prediction\n",
    "- `load_model_t` - use saved weights of model for value prediction (True) or overwrite with new data (False)\n",
    "- `cycles_e` - number of cycles in trainint model for value prediction\n",
    "- `load_model_e` - use saved weights of model for value prediction (True) or overwrite with new data (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "Trainer().create_models(names=names, window=50, cycles_t=0, load_model_t=True, cycles_e=0, load_model_e=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data for genetic algorithm\n",
    "\n",
    "After training models the Interval class will create prediction for the downloaded values in dataset and those will serve as an input to genetic algorithm, which will find the best strategy for trading based on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "for name in names:\n",
    "    Interval().generate_intervals(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic algorithm\n",
    "\n",
    "- `population_count` max population count in each generation\n",
    "- `top_count` number of surviving individuals from each generation\n",
    "- `generations_before_exit` how many generations will there be in total\n",
    "- `load_config` each generation the top individuals are saved in config file and can be loaded after start if this is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "Genetic(names).get_purchase_config(population_count=100, top_count=25, generations_before_exit=1000, load_configs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing transactions of the found configurations\n",
    "\n",
    "Following cell can be used after the genetic algorithm finds the best configurations. It will plot the financial gain after each transaction for all monitored cryptocurrencies so that user can manually choose which configuration should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "def cash_up(config, predictions):\n",
    "\n",
    "    transactions = 0.0\n",
    "    gain = 0.0\n",
    "    \n",
    "    rising = config[0]\n",
    "    sinking = config[1]\n",
    "    time_buy_rising = config[2]\n",
    "    time_buy_sinking = config[3]\n",
    "    time_sell_rising = config[4]\n",
    "    time_sell_sinking = config[5]\n",
    "\n",
    "    stat = list()\n",
    "    purchases_arr = list()\n",
    "\n",
    "    for prediction in predictions:\n",
    "\n",
    "        cash =  1.0\n",
    "        bought = False\n",
    "        bought_value = 0.0\n",
    "        purchases = list()\n",
    "\n",
    "        for interval in prediction:\n",
    "                \n",
    "            growth = interval[0]\n",
    "            duration = interval[1]\n",
    "            current_price = interval[2]\n",
    "            advice = 'wait'\n",
    "\n",
    "            if growth > rising:\n",
    "                if duration > time_buy_rising:\n",
    "                    advice = 'buy'\n",
    "                if duration <= time_sell_rising:\n",
    "                    advice = 'sell'\n",
    "            if growth <= sinking:\n",
    "                if duration < time_buy_sinking:\n",
    "                    advice = 'buy'\n",
    "                if duration >= time_sell_sinking:\n",
    "                    advice = 'sell'\n",
    "\n",
    "            if not bought and advice == 'buy':\n",
    "                bought = True\n",
    "                bought_value = current_price\n",
    "                deposited = cash\n",
    "                #print(f'BUY: {list(interval)}, CASH: {cash}, CURRENT_PRICE: {current_price}, BOUGHT_VALUE: {bought_value}, DEPOSITED: {deposited}')\n",
    "            if bought and advice == 'sell':\n",
    "                bought = False\n",
    "                transactions += 1\n",
    "                cash = (current_price / bought_value) * deposited\n",
    "                purchases.append(cash)\n",
    "                #print(f'SELL: {list(interval)}, CASH: {cash}, CURRENT_PRICE: {current_price}, BOUGHT_VALUE: {bought_value}, DEPOSITED: {deposited}')\n",
    "\n",
    "        purchases_arr.append(purchases)\n",
    "        gain += (cash - 1)\n",
    "        stat.append(round(cash - 1, 4))\n",
    "\n",
    "    for x in stat:\n",
    "        if x < 0:\n",
    "            gain = 0\n",
    "    \n",
    "    return gain, (transactions, tuple(stat)), purchases_arr\n",
    "\n",
    "def get_dataset(name):\n",
    "    with open(f'/workspace/data/interval_{name.lower()}.json', 'r') as file:\n",
    "        data = np.array(json.load(file), dtype=float)\n",
    "    return data\n",
    "\n",
    "def get_datasets(names):\n",
    "    return [get_dataset(name) for name in names]\n",
    "\n",
    "def plot_config(config, predictions):\n",
    "\n",
    "\n",
    "    gain , stat, evolutions = cash_up(config, predictions)\n",
    "\n",
    "    fig, axs = plt.subplots(len(evolutions), figsize=(18.5, 18.5))\n",
    "    fig.text(0.5, 0.04, 'Transakce', ha='center')\n",
    "    fig.text(0.04, 0.5, 'Zisk', va='center', rotation='vertical')\n",
    "\n",
    "    for i in range(len(evolutions)):\n",
    "        axs[i].plot(evolutions[i])\n",
    "\n",
    "    print(f'GAIN: {gain}, TRANSACTIONS: {int(stat[0])}, GAIN FOR EACH CRYPTO: {stat[1]}, CONFIG: {config}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "predictions = get_datasets(Database().get_names())\n",
    "\n",
    "with open(f'/workspace/data/config.json', 'r') as file:\n",
    "    configs = np.array([ np.array(x[1], dtype=float) for x in json.load(file)[::-1][:40]])\n",
    "\n",
    "for config in configs:\n",
    "    plot_config(config, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of local extremes\n",
    "\n",
    "This is an example of findin local extremes in our data. Note that user needs to specify cryptocurrency name and symbol in gui. If the scrape docker container is not running, the user needs to execute `InvestReq(60, 5).loop()`. So that data for visualization can be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "def plot(df, extremes, g_a, selected_name):\n",
    "    \n",
    "    x = extremes\n",
    "    y = arr[x]\n",
    "\n",
    "    plt.figure(figsize=(18.5, 10.5))\n",
    "    plt.xlabel('Minuty')\n",
    "    plt.ylabel('Hodnota')\n",
    "    plt.title(selected_name)\n",
    "    plt.plot(pd.DataFrame(df))\n",
    "    plt.plot(g_a)\n",
    "\n",
    "    plt.scatter(x,y,s=70,color='green',zorder=2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "selected_name = names[0]\n",
    "df = Database().select_all_from_and_fix_missing(selected_name, 'crypto', 'linear', limit_area='inside').head(5000).tail(1000)\n",
    "df_open = df['count'].values\n",
    "arr = df_open\n",
    "df = pd.DataFrame(arr)\n",
    "extremes, g_a = Minmax().find_extremes(np.array(df[0], dtype='float'), False)\n",
    "plot(df, extremes, g_a, selected_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity of the data\n",
    "\n",
    "The data needs to be stationary for better prediction accuracy. In the following cells we conduct visual and algorithmic check for stationarity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "def describe_train_set(names):\n",
    "\n",
    "    for name in names:\n",
    "        df, _ = Extrema().get_prediction_set(name)\n",
    "        df = df.astype(float)\n",
    "        df.plot(subplots=True, layout=(7, 3), figsize=(20, 40), sharex=False, kind='kde', title=f'{name} extremes density', grid=True)\n",
    "        df.plot(subplots=True, figsize=(20, 40), sharex=False, kind='line', title=f'{name} extremes in time', grid=True)\n",
    "\n",
    "describe_train_set([names[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Database().get_names()\n",
    "\n",
    "print('Null Hypothesis H0 = If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary')\n",
    "print('Alternative Hypothesis H1 = The null hypothesis is rejected and suggests the time series does not have a unit root, meaning it is stationary')\n",
    "print()\n",
    "\n",
    "def ADF_Cal(x, name):\n",
    "    print(f'======================={name}=======================')\n",
    "    result = adfuller(x)\n",
    "    ADF_stat = result[0]\n",
    "    p = result[1]\n",
    "    print(f'ADF Statistic: {ADF_stat}')\n",
    "    print(f'p-value: {p}')\n",
    "    print('Critical Values')\n",
    "    levels = [.1 , .05 , .01]\n",
    "    i = 0\n",
    "    for key ,value in result[4].items():\n",
    "        print(f'{key}: {value}')\n",
    "        hyp = p < levels[i]\n",
    "        if ADF_stat < value :\n",
    "            cert = (1 - levels [ i ])*100\n",
    "            print(f'{cert}% certain this is staionary')\n",
    "            print(f'Reject H0: {hyp}')\n",
    "            break\n",
    "        i = i +1\n",
    "        if i >= 3:\n",
    "            print(f'Less than 90% certain that data is stationary')\n",
    "            print(f'Reject H0: {hyp}')\n",
    "    print()\n",
    "\n",
    "\n",
    "df, _ = Extrema().get_prediction_set(names[0])\n",
    "for name in df.columns:\n",
    "    df = df.astype(float)\n",
    "    ADF_Cal(df['interval_60'].values, name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
